{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79303e0d-f8fd-4c8f-b339-e5a634b3440f",
   "metadata": {},
   "source": [
    "Kaggle's Titanic Dataset Predictions in Haskell\n",
    "===================================================\n",
    "\n",
    "This notebook shows a simple example of an analysis of the Titanic disaster in Haskell using the existing dataHaskell utilities as of November 2025. This is primariliy aimed at those who already have a working knowledge of data analytics and data science in Python/R, and who are looking to switch to Haskell!\n",
    "\n",
    "What This Program Does\n",
    "----------------------\n",
    "\n",
    "1. Loads the canonical [Titanic dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset)\n",
    "2. Describes the columns within the Titanic dataset\n",
    "4. Builds a logistic regression with Hasktorch\n",
    "5. Evaluates the performance of the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ceaae-5f46-4b80-8743-c54a99360b26",
   "metadata": {},
   "source": [
    "## Imports, etc\n",
    "\n",
    "1. We have to set `-XOverloadedStrings` because want to be able to refer to Haskell's `T.Text` type as regular `String`s, i,e. as sequences of characters between double-quotes: `\"this is both a T.Text and a String!\"`\n",
    "2. The `pandas` equivalent in Haskell is `DataFrame`.\n",
    "3. We import the pipe operator `|>`, similar to `%>%` in `dplyr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdfd0b-e440-413d-b5a7-6a783be1b433",
   "metadata": {},
   "outputs": [],
   "source": [
    ":set -XOverloadedStrings\n",
    "\n",
    "import qualified DataFrame as D\n",
    "import qualified DataFrame.Functions as F\n",
    "import           DataFrame.Operators\n",
    "\n",
    "import qualified Data.Text as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad72ef-64a5-4931-b1f3-9698dd25c714",
   "metadata": {},
   "source": [
    "## Reading in our dataset\n",
    "\n",
    "1. `DataFrame` is smart enough to understand what types the underlying CSV correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae01e2-06b8-4e91-be18-c7f8b6a4d4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
       "| PassengerId<br>Int | Survived<br>Int | Pclass<br>Int |                    Name<br>Text                     | Sex<br>Text | Age<br>Maybe Double | SibSp<br>Int | Parch<br>Int |  Ticket<br>Text  | Fare<br>Double | Cabin<br>Maybe Text | Embarked<br>Maybe Text |\n",
       "| -------------------|-----------------|---------------|-----------------------------------------------------|-------------|---------------------|--------------|--------------|------------------|----------------|---------------------|----------------------- |\n",
       "| 1                  | 0               | 3             | Braund, Mr. Owen Harris                             | male        | Just 22.0           | 1            | 0            | A/5 21171        | 7.25           | Nothing             | Just \"S\"               |\n",
       "| 2                  | 1               | 1             | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female      | Just 38.0           | 1            | 0            | PC 17599         | 71.2833        | Just \"C85\"          | Just \"C\"               |\n",
       "| 3                  | 1               | 3             | Heikkinen, Miss. Laina                              | female      | Just 26.0           | 0            | 0            | STON/O2. 3101282 | 7.925          | Nothing             | Just \"S\"               |\n",
       "| 4                  | 1               | 1             | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female      | Just 35.0           | 1            | 0            | 113803           | 53.1           | Just \"C123\"         | Just \"S\"               |\n",
       "| 5                  | 0               | 3             | Allen, Mr. William Henry                            | male        | Just 35.0           | 0            | 0            | 373450           | 8.05           | Nothing             | Just \"S\"               |\n"
      ],
      "text/plain": [
       "  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
       "| PassengerId<br>Int | Survived<br>Int | Pclass<br>Int |                    Name<br>Text                     | Sex<br>Text | Age<br>Maybe Double | SibSp<br>Int | Parch<br>Int |  Ticket<br>Text  | Fare<br>Double | Cabin<br>Maybe Text | Embarked<br>Maybe Text |\n",
       "| -------------------|-----------------|---------------|-----------------------------------------------------|-------------|---------------------|--------------|--------------|------------------|----------------|---------------------|----------------------- |\n",
       "| 1                  | 0               | 3             | Braund, Mr. Owen Harris                             | male        | Just 22.0           | 1            | 0            | A/5 21171        | 7.25           | Nothing             | Just \"S\"               |\n",
       "| 2                  | 1               | 1             | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female      | Just 38.0           | 1            | 0            | PC 17599         | 71.2833        | Just \"C85\"          | Just \"C\"               |\n",
       "| 3                  | 1               | 3             | Heikkinen, Miss. Laina                              | female      | Just 26.0           | 0            | 0            | STON/O2. 3101282 | 7.925          | Nothing             | Just \"S\"               |\n",
       "| 4                  | 1               | 1             | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female      | Just 35.0           | 1            | 0            | 113803           | 53.1           | Just \"C123\"         | Just \"S\"               |\n",
       "| 5                  | 0               | 3             | Allen, Mr. William Henry                            | male        | Just 35.0           | 0            | 0            | 373450           | 8.05           | Nothing             | Just \"S\"               |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  ------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
       "| Statistic<br>Text | PassengerId<br>Double | Survived<br>Double | Pclass<br>Double | Age<br>Double | SibSp<br>Double | Parch<br>Double | Fare<br>Double |\n",
       "| ------------------|-----------------------|--------------------|------------------|---------------|-----------------|-----------------|--------------- |\n",
       "| Count             | 891.0                 | 891.0              | 891.0            | 714.0         | 891.0           | 891.0           | 891.0          |\n",
       "| Mean              | 446.0                 | 0.38               | 2.31             | 29.7          | 0.52            | 0.38            | 32.2           |\n",
       "| Minimum           | 1.0                   | 0.0                | 1.0              | 0.42          | 0.0             | 0.0             | 0.0            |\n",
       "| 25%               | 223.5                 | 0.0                | 2.0              | 20.12         | 0.0             | 0.0             | 7.91           |\n",
       "| Median            | 446.0                 | 0.0                | 3.0              | 28.0          | 0.0             | 0.0             | 14.45          |\n",
       "| 75%               | 668.5                 | 1.0                | 3.0              | 38.0          | 1.0             | 0.0             | 31.0           |\n",
       "| Max               | 891.0                 | 1.0                | 3.0              | 80.0          | 8.0             | 6.0             | 512.33         |\n",
       "| StdDev            | 257.35                | 0.49               | 0.84             | 14.53         | 1.1             | 0.81            | 49.69          |\n",
       "| IQR               | 445.0                 | 1.0                | 1.0              | 17.88         | 1.0             | 0.0             | 23.09          |\n",
       "| Skewness          | 0.0                   | 0.48               | -0.63            | 0.39          | 3.69            | 2.74            | 4.78           |\n"
      ],
      "text/plain": [
       "  ------------------------------------------------------------------------------------------------------------------------------------------------------  \n",
       "| Statistic<br>Text | PassengerId<br>Double | Survived<br>Double | Pclass<br>Double | Age<br>Double | SibSp<br>Double | Parch<br>Double | Fare<br>Double |\n",
       "| ------------------|-----------------------|--------------------|------------------|---------------|-----------------|-----------------|--------------- |\n",
       "| Count             | 891.0                 | 891.0              | 891.0            | 714.0         | 891.0           | 891.0           | 891.0          |\n",
       "| Mean              | 446.0                 | 0.38               | 2.31             | 29.7          | 0.52            | 0.38            | 32.2           |\n",
       "| Minimum           | 1.0                   | 0.0                | 1.0              | 0.42          | 0.0             | 0.0             | 0.0            |\n",
       "| 25%               | 223.5                 | 0.0                | 2.0              | 20.12         | 0.0             | 0.0             | 7.91           |\n",
       "| Median            | 446.0                 | 0.0                | 3.0              | 28.0          | 0.0             | 0.0             | 14.45          |\n",
       "| 75%               | 668.5                 | 1.0                | 3.0              | 38.0          | 1.0             | 0.0             | 31.0           |\n",
       "| Max               | 891.0                 | 1.0                | 3.0              | 80.0          | 8.0             | 6.0             | 512.33         |\n",
       "| StdDev            | 257.35                | 0.49               | 0.84             | 14.53         | 1.1             | 0.81            | 49.69          |\n",
       "| IQR               | 445.0                 | 1.0                | 1.0              | 17.88         | 1.0             | 0.0             | 23.09          |\n",
       "| Skewness          | 0.0                   | 0.48               | -0.63            | 0.39          | 3.69            | 2.74            | 4.78           |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- Since we have imported qualified DataFrame as D, we must refer to \n",
    "-- functions inside the DataFrame library with the prefix `D.`\n",
    "\n",
    "df <- D.readCsv \"../data/titanic.csv\"\n",
    "\n",
    "-- by the way, you can extract the first N rows using D.take\n",
    "df |> D.take 5\n",
    "\n",
    "-- and you can get summary stats using D.summarize\n",
    "\n",
    "df |> D.summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe73b2-6958-4f84-9a82-238c25a598ad",
   "metadata": {},
   "source": [
    "# Prepping our data for Hasktorch\n",
    "\n",
    "Here are some things that we can observe about our dataset:\n",
    "\n",
    "1. `Embarked` represents the port where the person boarded. There's no reason why a person boarding in France would have a better chance than a person boarding in England, so we will drop this column.\n",
    "2. `Cabin` represents the room number the passenger was assigned. Not all passengers were assigned a cabin. Since this is correlated with `Pclass`, we'll drop this column.\n",
    "3. `Fare` is the amount the passenger paid for their ticket. This is also correlated with `Pclass`. In order to reduce the amount of variance in the dataset, we'll use `Pclass` and drop this column.\n",
    "4. `Parch` refers to the number of parents or children the individual had riding with them. We'll ignore this column for the time being and leave the inclusion of this as an exercise.\n",
    "5. `SipSp` refers to the number of siblings or spouses the individual had riding with them. We'll ignore this column for the time being and leave the inclusion of this as an exercise.\n",
    "6. `Age` - self explanatory. We will impute the average age in order to handle missing values.\n",
    "7. `Sex` - self explanatory.\n",
    "8. `Pclass` refers to the class of the ticket: first, second or third. Though this is read in as an `Int`, it can't be treated as an `Int`, otherwise our model would think that the value of third class is three times the value of first class, which in general is not true (first class tickets are not 1/3 the price of third class tickets, for example). Therefore, we will convert this into two binary variables, `isFirstClass` and `isSecondClass`. The default will be the passenger belongs to third class, unless either of these variables is set to 1. \n",
    "9. `Survived` is our outcome variable of interest.\n",
    "10. `PassengerId` has to be dropped as it is an ID.\n",
    "\n",
    "\n",
    "The outcome of interest is the column called `Survived`. Let's compare between two logistic regression models:\n",
    "\n",
    "1. _Minimal model_: The only independent variables is `Sex`\n",
    "2. _Bigger model_: The independent variables are `Sex`, class (represented by binary variables `isFirstClass` and `isSecondClass` and `Age`.\n",
    "\n",
    "Let's start by prepping the data. \n",
    "\n",
    "1. We'll binarize the `Sex` and `Pclass` columns\n",
    "2. We'll impute missing `Age` values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc7b0553-0a21-49af-a889-f55b38e8b652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.69911764705882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- let's compute the mean age. We can exclude\n",
    "-- the null rows using the function `filterJust`.\n",
    "-- (Similarly, you could exclude the nonnull rows\n",
    "-- using the function `filterNothing`)\n",
    "\n",
    "\n",
    "df |> D.filterJust \"Age\" |> D.mean (F.col @Double \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50baa76c-c3e3-4988-ab1e-051c2b58d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  ----------------------------------------------------------------------------------------------  \n",
       "| Age<br>Double | Survived<br>Int | isFemale<br>Int | isFirstClass<br>Int | isSecondClass<br>Int |\n",
       "| --------------|-----------------|-----------------|---------------------|--------------------- |\n",
       "| 22.0          | 0               | 0               | 0                   | 0                    |\n",
       "| 38.0          | 1               | 1               | 1                   | 0                    |\n",
       "| 26.0          | 1               | 1               | 0                   | 0                    |\n",
       "| 35.0          | 1               | 1               | 1                   | 0                    |\n",
       "| 35.0          | 0               | 0               | 0                   | 0                    |\n"
      ],
      "text/plain": [
       "  ----------------------------------------------------------------------------------------------  \n",
       "| Age<br>Double | Survived<br>Int | isFemale<br>Int | isFirstClass<br>Int | isSecondClass<br>Int |\n",
       "| --------------|-----------------|-----------------|---------------------|--------------------- |\n",
       "| 22.0          | 0               | 0               | 0                   | 0                    |\n",
       "| 38.0          | 1               | 1               | 1                   | 0                    |\n",
       "| 26.0          | 1               | 1               | 0                   | 0                    |\n",
       "| 35.0          | 1               | 1               | 1                   | 0                    |\n",
       "| 35.0          | 0               | 0               | 0                   | 0                    |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleanedDf = df |> D.select [\"Sex\", \"Pclass\", \"Age\", \"Survived\"]\n",
    "               |> D.derive \"isFemale\" (F.ifThenElse (F.col \"Sex\" .== \"female\") 1 0)\n",
    "               |> D.derive \"isFirstClass\" (F.ifThenElse (F.col \"Pclass\" .== 1) 1 0)\n",
    "               |> D.derive \"isSecondClass\" (F.ifThenElse (F.col \"Pclass\" .== 2) 1 0)\n",
    "               |> D.impute (F.col \"Age\") 29.69911764705882\n",
    "               |> D.exclude [\"Sex\", \"Pclass\"]\n",
    "\n",
    "cleanedDf |> D.take 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a3294-5006-4a5c-b13f-9db6c00a86dd",
   "metadata": {},
   "source": [
    "# Logistic Regression in Hasktorch\n",
    "\n",
    "Let's take a brief detour and explore how we would build a logistic regression using the functionality given to us in Torch.\n",
    "\n",
    "- A logistic regression is used whenever the output is a binary variable. It is a generalized linear regression model that uses the logit link.\n",
    "- This can be expressed in torch as a linear model with no hidden features. The input features are forwarded to the output features with the sigmoid function (inverse of the logit link).\n",
    "- There is one output, which can be interpreted as the probability that the output (a binary variable) is \"successful\".\n",
    "\n",
    "Let's first define this architecture using Hasktorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09ffb51f-dc32-4bbe-9cdc-005dd1efe51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE DeriveGeneric #-}\n",
    "{-# LANGUAGE RecordWildCards #-}\n",
    "\n",
    "import GHC.Generics (Generic)\n",
    "import qualified Torch as HT\n",
    "import Control.Monad (when)\n",
    "\n",
    "-- Forward pass\n",
    "-- similar to the following code in PyTorch\n",
    "-- def forward(self, x):\n",
    "--        out = self.linear(x) # Pass the input through the linear layer\n",
    "--        out = self.sigmoid(out) # Apply the sigmoid activation function\n",
    "--        return out\n",
    "\n",
    "logReg :: HT.Linear -> HT.Tensor -> HT.Tensor\n",
    "logReg wtsbs input = HT.squeezeAll $ HT.sigmoid $ HT.linear wtsbs input\n",
    "\n",
    "-- here is a helper function that runs a training loop.\n",
    "-- similar to `for epoch in range(1000)...` in PyTorch.\n",
    "\n",
    "trainLoop :: \n",
    "    Int ->\n",
    "    (HT.Tensor, HT.Tensor) ->\n",
    "    HT.Linear ->\n",
    "    IO HT.Linear\n",
    "trainLoop \n",
    "    n\n",
    "    (features, labels)\n",
    "    initialM =\n",
    "        HT.foldLoop initialM n $ \\state i -> do\n",
    "            -- Forward pass: compute predictions\n",
    "            let predicted = logReg state features\n",
    "            \n",
    "            -- Compute loss (how wrong our predictions are)\n",
    "            let loss = HT.binaryCrossEntropyLoss' labels predicted\n",
    "            \n",
    "            -- Every 1000 iterations, print progress\n",
    "            when (i `mod` 500 == 0) $ do\n",
    "                putStrLn $\n",
    "                    \"Iteration :\"\n",
    "                        ++ show i\n",
    "                        ++ \" | Loss: \"\n",
    "                        ++ show (HT.asValue loss :: Float)\n",
    "            \n",
    "            -- Backward pass: update weights using gradient descent\n",
    "            -- HT.GD is the optimizer, 1e-2 is the learning rate\n",
    "            (state', _) <- HT.runStep state HT.GD loss 1e-2\n",
    "            pure state'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5c2be-3bd9-4e6a-bd0c-f066b3e768ae",
   "metadata": {},
   "source": [
    "# Converting Dataframes to Tensors\n",
    "\n",
    "Hasktorch works with Tensors, so we need to convert the Data to Tensors. We can use the Hasktorch utils in the Dataframe library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc2f0f69-5cf3-438e-8f48-68ce0667d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qualified DataFrame.Hasktorch as DHT\n",
    "\n",
    "labelsTensor = cleanedDf |> D.select [\"Survived\"] \n",
    "                         |> DHT.toTensor\n",
    "\n",
    "featuresTensor = cleanedDf |> D.exclude [\"Survived\"]\n",
    "                           |> DHT.toTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412f680-c5db-48e6-a51c-245b685f4042",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "\n",
    "We're now ready to run the models:\n",
    "\n",
    "1. First, initialize the models randomly.\n",
    "2. Then, using the `trainLoop` function we wrote above, train the model for a certain number of epochs.\n",
    "\n",
    "To make things easy, we'll just use 10,000 epochs. But you can modify `trainLoop` to do fancier things such as stopping only after the average error over 100 epochs is not improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3132fea-08bd-44c4-be8b-1d7065ccc33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iteration :500 | Loss: 0.6940311\n",
       "Iteration :1000 | Loss: 0.62007225\n",
       "Iteration :1500 | Loss: 0.5619386\n",
       "Iteration :2000 | Loss: 0.5133588\n",
       "Iteration :2500 | Loss: 0.49150816\n",
       "Iteration :3000 | Loss: 0.48291054\n",
       "Iteration :3500 | Loss: 0.47661462\n",
       "Iteration :4000 | Loss: 0.47188118\n",
       "Iteration :4500 | Loss: 0.46824375\n",
       "Iteration :5000 | Loss: 0.46539697\n",
       "Iteration :5500 | Loss: 0.46313477\n",
       "Iteration :6000 | Loss: 0.46131372\n",
       "Iteration :6500 | Loss: 0.45983192\n",
       "Iteration :7000 | Loss: 0.45861503\n",
       "Iteration :7500 | Loss: 0.45760784\n",
       "Iteration :8000 | Loss: 0.4567686\n",
       "Iteration :8500 | Loss: 0.4560653\n",
       "Iteration :9000 | Loss: 0.45547295\n",
       "Iteration :9500 | Loss: 0.4549719\n",
       "Iteration :10000 | Loss: 0.45454657\n",
       "Iteration :10500 | Loss: 0.4541842\n",
       "Iteration :11000 | Loss: 0.45387465\n",
       "Iteration :11500 | Loss: 0.45360953\n",
       "Iteration :12000 | Loss: 0.45338193\n",
       "Iteration :12500 | Loss: 0.45318606\n",
       "Iteration :13000 | Loss: 0.45301735\n",
       "Iteration :13500 | Loss: 0.45287165\n",
       "Iteration :14000 | Loss: 0.4527457\n",
       "Iteration :14500 | Loss: 0.4526367\n",
       "Iteration :15000 | Loss: 0.45254216\n",
       "Iteration :15500 | Loss: 0.45246017\n",
       "Iteration :16000 | Loss: 0.4523889\n",
       "Iteration :16500 | Loss: 0.452327\n",
       "Iteration :17000 | Loss: 0.4522731\n",
       "Iteration :17500 | Loss: 0.45222613\n",
       "Iteration :18000 | Loss: 0.4521853\n",
       "Iteration :18500 | Loss: 0.4521496\n",
       "Iteration :19000 | Loss: 0.45211846\n",
       "Iteration :19500 | Loss: 0.45209128\n",
       "Iteration :20000 | Loss: 0.45206752\n",
       "Iteration :20500 | Loss: 0.4520468\n",
       "Iteration :21000 | Loss: 0.45202863\n",
       "Iteration :21500 | Loss: 0.4520127\n",
       "Iteration :22000 | Loss: 0.45199886\n",
       "Iteration :22500 | Loss: 0.45198667\n",
       "Iteration :23000 | Loss: 0.45197603\n",
       "Iteration :23500 | Loss: 0.4519667\n",
       "Iteration :24000 | Loss: 0.4519585\n",
       "Iteration :24500 | Loss: 0.45195135\n",
       "Iteration :25000 | Loss: 0.45194507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initialModel <- HT.sample $ HT.LinearSpec 4 1\n",
    "\n",
    "trainedModel <- trainLoop 25000 (featuresTensor, labelsTensor) initialModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cc716-44a8-40fc-a61b-395e503ce8f2",
   "metadata": {},
   "source": [
    "# Using the model to generate predictions\n",
    "\n",
    "We now have a model that we can use to generate predictions on whether a given passenger survived. But note that our predictions are values between 0 and 1! We interpret this as the probability that a given passenger would survive. \n",
    "\n",
    "As a first step, we can just have a threshold of 0.5: if the probability is more than 50%, then we predict the passenger to survive, otherwise, we predict them to not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "349e9477-94c2-4a6a-8aac-535ea236d50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8013468"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Data.Either (fromRight)\n",
    "import qualified Data.Vector.Unboxed as VU\n",
    "\n",
    "getValues :: HT.Tensor -> [Float]\n",
    "getValues tsr = HT.asValue tsr :: [Float]\n",
    "\n",
    "assignSurvival :: Float -> [Float] -> [Int]\n",
    "assignSurvival threshold = fmap (\\ p -> if p > threshold then 1 else 0)\n",
    "\n",
    "computeAccuracy :: [Int] -> [Int] -> Float\n",
    "computeAccuracy actual predicted = fromIntegral numCorrect / fromIntegral numObs\n",
    "    where correct = zipWith (\\ a b -> if a == b then 1 else 0) actual predicted\n",
    "          numCorrect = sum correct\n",
    "          numObs = length actual\n",
    "\n",
    "predictedProbsTensor = logReg trainedModel featuresTensor\n",
    "predictedSurvival = assignSurvival 0.5 $ getValues predictedProbsTensor\n",
    "accuracy = computeAccuracy predictedSurvival (VU.toList $ fromRight undefined (df |> D.columnAsIntVector \"Survived\"))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15a306-59e7-4b56-896d-50f8c574437a",
   "metadata": {},
   "source": [
    "# Conclusions and Next Steps\n",
    "\n",
    "Great! We now have a preliminary model where we have about 80% predictive accuracy on the underlying dataset.\n",
    "\n",
    "Here are a couple of ideas on what you can do next with Haskell's dataframes\n",
    "\n",
    "1. This model is a linear model, so you'll need to introduce nonlinearities manually -- for example, it's plausible that age squared might be better correlated with the outcome, as the very young and the very old might have a lower probability of survival than those in their 20s/30s. Can you add a new column, `ageSq`?\n",
    "2. Can you compute other metrics such as precision and recall? The `iris` example notebook has code snippets that you can use.\n",
    "3. Download the test set from Kaggle. Generate the predictions and submit it to Kaggle. What's the accuracy that you get?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "9.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
